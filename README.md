# Movie Recommender System

by Dmitry Beresnev
IU F23 / B20-AI / <d.beresnev@innopolis.university>

## Introduction

This repository is the solution for Assignment 2 of Practical Machine Learning and Deep Learning F23 IU course

## Requirements

Code was tested on Windows 11, Python 3.11 and CUDA 11.8.

All the requirement packages are listed in the file `requirements.txt`. In case you use the `pipenv` package, there is also `Pipfile` in the root of the project.

## Before start

Install all the packages from _requirements.txt_ using `pip install -r requirements.txt` or using **pipenv** `pipenv install`.

Optionally, you can run `bash setup_precommit.sh` to setup pre-commit hook for GitHub for code formatting using [ruff](https://docs.astral.sh/ruff/).

I also highly recommend to read reports in corresponding `reports` folder to fully understand context and purpose of some files and folders.

## Repository structure

```text
├── README.md       # The top-level README
│
├── benchmark               # Measuring model performance
│   ├── evaluate.py         # Script for model performance evaluation
│   ├── interactive.py      # Script for real-time interaction with model
|   |
│   └── data                # Performance result and some datasets
|       ├── generated       # `./benchmark/evaluate.py` results as .json and .svg files
|       |
|       ├── masks_split
|       |   └── test        # Sample benchmark data folder. Check report for more info on splits
|       |
|       └── users_split
|           └── test        # Sample benchmark data folder. Check report for more info on splits
|
├── data
│   ├── external            # Data from third party sources
|   |
│   ├── interim             # Datasets generated by notebooks and scripts
|   |   ├── masks_split     # Final dataset split on mask
|   |   |   ├── test
|   |   |   └── train
|   |   |
|   |   ├── users_split     # Final dataset split on users
|   |   |   ├── test
|   |   |   └── train
|   |   |
|   |   ├── encoded_users.csv         # Dataset generated on first EDA iteration
|   |   └── users_with_masks.csv      # Masked dataset generated on first EDA iteration
|   |
│   ├── raw                 # Data generated by models and other scripts
|   |   └── ml-100k         # Initial base MovieLens-100K dataset
|   |
│   └── results             # Metrics results generated by models in notebooks
|       ├── comparison.svg            # Plot with metrics of different solution models
|       ├── tunned_comparison.svg     # Plot with metrics of tunned best solution models
|       ├── models_info.json          # Metrics and training information of different solution models
|       └── tunned_models_info.json   # Metrics and training information of tuned solution models
│
├── models          # Trained and serialized models, final checkpoints
│   └── {tunned}_{loss}_{split}_{masked}_{checkpoint}   # Many models with such naming convention, where
|       ├── {tunned}      # Is model a tunned one. Can be omitted
|       ├── {loss}        # Loss used during training. `mse` or `bce`
|       ├── {split}       # Dataset split type. `msplit` stands for masks split, `usplit` - for users split
|       ├── {masked}      # Was loss computed on masked data. `m` if yes, omitted if no
|       └── {checkpoint}  # `latest` or `best` (on validation data)
│
├── notebooks       #  Jupyter notebooks
│   ├── 1.0-data-exploration.ipynb      # First EDA iteration
│   ├── 1.1-data-exploration.ipynb      # Second and final EDA iteration
│   ├── 2.0-models-comparison.ipynb     # Comparison of different solution models
│   └── 3.0-final-models-tunning.ipynb  # Tunning of the best models based on comparison
│
├── references      # Data dictionaries, manuals, and all other explanatory materials
│
├── reports         # Generated analysis as HTML, PDF, LaTeX, etc.
│   ├── Report.md           # Main assignment report
│   └── figures             # Generated graphics and figures to be used in reporting
│
├── requirements.txt  # The requirements file for reproducing the analysis environment
│                      generated with `pip freeze › requirements. txt`
|
├── AssignmentDescription.md  # Description of the task
├── Pipfile                   # File with dependencies for pipenv
├── Pipfile.lock              # File with locked dependencies for pipenv
├── pyproject.toml            # Formatter and linter settings
└── setup_precommit.sh        # Script for creating pre-commit GitHub hook
```

## Basic usage

This section briefly describes how to use scripts from `benchmark/` folder.

For all scripts help messages are available with `-h` flag. For example, `python ./benchmark/evaluate.py -h` explains all the available flags and their purpose.
Generally, for all scripts two modes are available: verbose and non-verbose.
By default verbose mode is active, and to run the script in silent mode you need the `--no-verbose` flag.

Generally, you can run each script with no flags at all. However, I **highly recommend** to always read the help messages before using scripts.

`./benchmark/evaluate.py` script is used for model performance evaluation.
You can specify model (by path) and data (also by path) for evaluation. Note, that by default script interprets data path
as path to the folder with several .csv files. If you want to pass single file, enable file mode by `-f` flag.
Be default, resulting data is saved to `./benchmark/data/generated/`.

`./benchmark/interactive.py` script is used for real-time interaction with model.
You can specify user parameters, such as age (i.e. `-a 21`),
occupation (i.e. `-o 19` for student),
gender (i.e. `-g 1` for male)
and favorite movies (i.e. `-f 1 56` for "Toy Story" and "Pulp Fiction") to get new movies recommendation.

## References

### Metrics

- [Retrieval precision on K](https://pytorch.org/torcheval/main/generated/torcheval.metrics.functional.retrieval_precision.html)
- [MAP@K](https://machinelearninginterview.com/topics/machine-learning/mapatk_evaluation_metric_for_ranking/)

### Datasets

- [MovieLens 100K dataset](https://grouplens.org/datasets/movielens/100k/)

## Contacts

In case of any questions you can contact me via email <d.beresnev@innopolis.university> or Telegram **@flip_floppa**
