{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rating BCE loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T15:45:44.873357Z",
     "iopub.status.busy": "2023-10-30T15:45:44.873056Z",
     "iopub.status.idle": "2023-10-30T15:45:50.512221Z",
     "shell.execute_reply": "2023-10-30T15:45:50.511415Z",
     "shell.execute_reply.started": "2023-10-30T15:45:44.873330Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import warnings\n",
    "from ast import literal_eval\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torcheval.metrics.functional.ranking import retrieval_precision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T15:45:50.514346Z",
     "iopub.status.busy": "2023-10-30T15:45:50.513886Z",
     "iopub.status.idle": "2023-10-30T15:45:50.522654Z",
     "shell.execute_reply": "2023-10-30T15:45:50.521758Z",
     "shell.execute_reply.started": "2023-10-30T15:45:50.514319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MANUAL_SEED = 42\n",
    "torch.manual_seed(MANUAL_SEED)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    loaded_dfs = [\n",
    "        pd.read_csv(os.path.join(path, file_name)) for file_name in os.listdir(path)\n",
    "    ]\n",
    "    return pd.concat(loaded_dfs)\n",
    "\n",
    "\n",
    "def load_datasets(path: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    return load_dataset(os.path.join(path, \"train/\")), load_dataset(\n",
    "        os.path.join(path, \"test/\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_df)=22632\n",
      "len(val_df)=2829\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = load_datasets(\"../data/interim/masks_split/\")\n",
    "\n",
    "print(f\"{len(train_df)=}\")\n",
    "print(f\"{len(val_df)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MOVIES = 1682\n",
    "BASIC_USER_FEATURES = 3\n",
    "\n",
    "TOTAL_USER_FEATURES = BASIC_USER_FEATURES + 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T15:46:04.094446Z",
     "iopub.status.busy": "2023-10-30T15:46:04.093174Z",
     "iopub.status.idle": "2023-10-30T15:46:04.105119Z",
     "shell.execute_reply": "2023-10-30T15:46:04.104195Z",
     "shell.execute_reply.started": "2023-10-30T15:46:04.094407Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RecommendationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.drop(columns=[\"user_id\"])\n",
    "        features = []\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            features.append(row[:3].tolist() + literal_eval(row[\"genres\"]))\n",
    "            inputs.append(literal_eval(row[\"input\"]))\n",
    "            targets.append(literal_eval(row[\"output\"]))\n",
    "\n",
    "        self.features = np.array(features)\n",
    "\n",
    "        # normalize ratings\n",
    "        self.inputs = np.array(inputs) / 5\n",
    "        self.targets = np.array(targets) / 5\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        input_ratings = self.inputs[idx]\n",
    "        input_data = np.concatenate([self.features[idx], input_ratings])\n",
    "        mask = input_ratings == 0\n",
    "        return input_data, mask, self.targets[idx]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T15:46:07.781101Z",
     "iopub.status.busy": "2023-10-30T15:46:07.780816Z",
     "iopub.status.idle": "2023-10-30T15:46:07.811091Z",
     "shell.execute_reply": "2023-10-30T15:46:07.810236Z",
     "shell.execute_reply.started": "2023-10-30T15:46:07.781077Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22632/22632 [04:16<00:00, 88.22it/s] \n",
      "100%|██████████| 2829/2829 [00:27<00:00, 101.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataset)=22632\n",
      "len(val_dataset)=2829\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = (\n",
    "    RecommendationDataset(train_df),\n",
    "    RecommendationDataset(val_df),\n",
    ")\n",
    "print(f\"{len(train_dataset)=}\")\n",
    "print(f\"{len(val_dataset)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T15:46:07.812402Z",
     "iopub.status.busy": "2023-10-30T15:46:07.812119Z",
     "iopub.status.idle": "2023-10-30T15:46:07.816776Z",
     "shell.execute_reply": "2023-10-30T15:46:07.815820Z",
     "shell.execute_reply.started": "2023-10-30T15:46:07.812377Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T15:46:07.831643Z",
     "iopub.status.busy": "2023-10-30T15:46:07.831399Z",
     "iopub.status.idle": "2023-10-30T15:46:07.842123Z",
     "shell.execute_reply": "2023-10-30T15:46:07.841338Z",
     "shell.execute_reply.started": "2023-10-30T15:46:07.831622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch: list) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    input_data_batch, mask_batch, target_batch = [], [], []\n",
    "    for input_data, mask, target in batch:\n",
    "        input_data_batch.append(input_data)\n",
    "        mask_batch.append(mask)\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return (\n",
    "        torch.Tensor(input_data_batch),\n",
    "        torch.Tensor(mask_batch).bool(),\n",
    "        torch.Tensor(target_batch),\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T15:46:07.921039Z",
     "iopub.status.busy": "2023-10-30T15:46:07.920762Z",
     "iopub.status.idle": "2023-10-30T15:46:07.948728Z",
     "shell.execute_reply": "2023-10-30T15:46:07.947826Z",
     "shell.execute_reply.started": "2023-10-30T15:46:07.921015Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1704])\n",
      "torch.Size([32, 1682])\n",
      "torch.Size([32, 1682])\n"
     ]
    }
   ],
   "source": [
    "it = train_dataloader._get_iterator()\n",
    "inp, mask, out = it._next_data()\n",
    "print(inp.shape)\n",
    "print(mask.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T15:46:07.950125Z",
     "iopub.status.busy": "2023-10-30T15:46:07.949851Z",
     "iopub.status.idle": "2023-10-30T15:46:07.967626Z",
     "shell.execute_reply": "2023-10-30T15:46:07.966641Z",
     "shell.execute_reply.started": "2023-10-30T15:46:07.950101Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = TOTAL_USER_FEATURES + NUM_MOVIES\n",
    "\n",
    "\n",
    "class RecSys(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim1: int = 1024,\n",
    "        hidden_dim2: int = 1024,\n",
    "    ):\n",
    "        super(RecSys, self).__init__()\n",
    "\n",
    "        self.d1 = nn.Dropout(0.1)\n",
    "\n",
    "        self.fc1 = nn.Linear(INPUT_SIZE, hidden_dim1)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_dim2, NUM_MOVIES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.d1(x)\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        return F.sigmoid(self.fc3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T15:46:13.722557Z",
     "iopub.status.busy": "2023-10-30T15:46:13.722268Z",
     "iopub.status.idle": "2023-10-30T15:46:14.170700Z",
     "shell.execute_reply": "2023-10-30T15:46:14.169643Z",
     "shell.execute_reply.started": "2023-10-30T15:46:13.722526Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(MANUAL_SEED)\n",
    "\n",
    "\n",
    "def create_model() -> tuple[nn.Module, Any]:\n",
    "    model = RecSys()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T15:46:14.172208Z",
     "iopub.status.busy": "2023-10-30T15:46:14.171914Z",
     "iopub.status.idle": "2023-10-30T15:46:14.185601Z",
     "shell.execute_reply": "2023-10-30T15:46:14.184636Z",
     "shell.execute_reply.started": "2023-10-30T15:46:14.172183Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: nn.Module, loader, optimizer, loss_fn, epoch, use_mask: bool = True\n",
    "):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(\n",
    "        loader,\n",
    "        total=len(loader),\n",
    "        desc=f\"Epoch {epoch}: train\",\n",
    "        leave=True,\n",
    "    )\n",
    "    for batch in loop:\n",
    "        input_data, mask, target = batch\n",
    "        input_data, target, mask = (\n",
    "            input_data.to(DEVICE),\n",
    "            target.to(DEVICE),\n",
    "            mask.to(DEVICE),\n",
    "        )\n",
    "\n",
    "        # forward pass and loss calculation\n",
    "        outputs = model(input_data)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        positive_targets = (target > 0).float()\n",
    "        if use_mask:\n",
    "            loss = loss_fn(\n",
    "                torch.masked_select(outputs, mask),\n",
    "                torch.masked_select(positive_targets, mask),\n",
    "            )\n",
    "        else:\n",
    "            loss = loss_fn(outputs, positive_targets)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        total += target.shape[1]\n",
    "\n",
    "        # optimizer run\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        loop.set_postfix({\"loss\": train_loss / total})\n",
    "\n",
    "\n",
    "def val_one_epoch(model: nn.Module, loader, loss_fn, epoch, use_mask: bool = True):\n",
    "    loop = tqdm(\n",
    "        loader,\n",
    "        total=len(loader),\n",
    "        desc=f\"Epoch {epoch}: val\",\n",
    "        leave=True,\n",
    "    )\n",
    "    val_loss = 0.0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # evaluation mode\n",
    "        for batch in loop:\n",
    "            input_data, mask, target = batch\n",
    "            input_data, target, mask = (\n",
    "                input_data.to(DEVICE),\n",
    "                target.to(DEVICE),\n",
    "                mask.to(DEVICE),\n",
    "            )\n",
    "\n",
    "            outputs = model(input_data)\n",
    "\n",
    "            positive_targets = (target > 0).float()\n",
    "            if use_mask:\n",
    "                loss = loss_fn(\n",
    "                    torch.masked_select(outputs, mask),\n",
    "                    torch.masked_select(positive_targets, mask),\n",
    "                )\n",
    "            else:\n",
    "                loss = loss_fn(outputs, positive_targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            total += target.shape[1]\n",
    "            loop.set_postfix({\"loss\": val_loss / total})\n",
    "    return val_loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    save_path: str,\n",
    "    use_mask: bool = True,\n",
    ") -> nn.Module:\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        train_one_epoch(\n",
    "            model, train_dataloader, optimizer, loss_fn, epoch, use_mask=use_mask\n",
    "        )\n",
    "        val_loss = val_one_epoch(model, val_dataloader, loss_fn, epoch, use_mask=use_mask)\n",
    "        if val_loss <= best_loss:\n",
    "            val_loss = best_loss\n",
    "            torch.save(model, save_path)\n",
    "\n",
    "    return copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T15:46:14.357029Z",
     "iopub.status.busy": "2023-10-30T15:46:14.356687Z",
     "iopub.status.idle": "2023-10-30T16:00:30.509074Z",
     "shell.execute_reply": "2023-10-30T16:00:30.508269Z",
     "shell.execute_reply.started": "2023-10-30T15:46:14.356996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model, optimizer = create_model()\n",
    "model_mask, optimizer_mask = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: train: 100%|██████████| 708/708 [00:38<00:00, 18.31it/s, loss=9.22e-5] \n",
      "Epoch 1: val: 100%|██████████| 89/89 [00:03<00:00, 23.18it/s, loss=8.21e-5]\n",
      "Epoch 2: train: 100%|██████████| 708/708 [00:40<00:00, 17.30it/s, loss=5.87e-5]\n",
      "Epoch 2: val: 100%|██████████| 89/89 [00:04<00:00, 21.37it/s, loss=6.28e-5]\n",
      "Epoch 3: train: 100%|██████████| 708/708 [00:38<00:00, 18.39it/s, loss=4.11e-5]\n",
      "Epoch 3: val: 100%|██████████| 89/89 [00:04<00:00, 19.97it/s, loss=5.2e-5] \n",
      "Epoch 4: train: 100%|██████████| 708/708 [00:42<00:00, 16.76it/s, loss=2.85e-5]\n",
      "Epoch 4: val: 100%|██████████| 89/89 [00:04<00:00, 21.89it/s, loss=4.73e-5]\n",
      "Epoch 5: train: 100%|██████████| 708/708 [00:41<00:00, 17.26it/s, loss=2.03e-5]\n",
      "Epoch 5: val: 100%|██████████| 89/89 [00:04<00:00, 19.67it/s, loss=4.12e-5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RecSys(\n",
       "  (d1): Dropout(p=0.1, inplace=False)\n",
       "  (fc1): Linear(in_features=1704, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc3): Linear(in_features=1024, out_features=1682, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = train_model(\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    \"../models/rating_bce\",\n",
    "    use_mask=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: train: 100%|██████████| 708/708 [00:43<00:00, 16.42it/s, loss=6.52e-5] \n",
      "Epoch 1: val: 100%|██████████| 89/89 [00:04<00:00, 18.46it/s, loss=7.54e-5]\n",
      "Epoch 2: train: 100%|██████████| 708/708 [00:43<00:00, 16.27it/s, loss=4.52e-5]\n",
      "Epoch 2: val: 100%|██████████| 89/89 [00:03<00:00, 22.48it/s, loss=6.65e-5]\n",
      "Epoch 3: train: 100%|██████████| 708/708 [00:39<00:00, 18.08it/s, loss=3.8e-5] \n",
      "Epoch 3: val: 100%|██████████| 89/89 [00:04<00:00, 21.10it/s, loss=6.17e-5]\n",
      "Epoch 4: train: 100%|██████████| 708/708 [00:40<00:00, 17.68it/s, loss=3.17e-5]\n",
      "Epoch 4: val: 100%|██████████| 89/89 [00:03<00:00, 24.57it/s, loss=5.29e-5]\n",
      "Epoch 5: train: 100%|██████████| 708/708 [00:43<00:00, 16.10it/s, loss=2.6e-5] \n",
      "Epoch 5: val: 100%|██████████| 89/89 [00:04<00:00, 19.62it/s, loss=5.09e-5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RecSys(\n",
       "  (d1): Dropout(p=0.1, inplace=False)\n",
       "  (fc1): Linear(in_features=1704, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc3): Linear(in_features=1024, out_features=1682, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_mask = train_model(\n",
    "    model_mask,\n",
    "    optimizer_mask,\n",
    "    loss_fn,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    \"../models/rating_bce_mask\",\n",
    "    use_mask=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T16:00:30.510782Z",
     "iopub.status.busy": "2023-10-30T16:00:30.510404Z",
     "iopub.status.idle": "2023-10-30T16:00:30.592431Z",
     "shell.execute_reply": "2023-10-30T16:00:30.591458Z",
     "shell.execute_reply.started": "2023-10-30T16:00:30.510749Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecSys(\n",
       "  (d1): Dropout(p=0.1, inplace=False)\n",
       "  (fc1): Linear(in_features=1704, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc3): Linear(in_features=1024, out_features=1682, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(\"../models/rating_bce\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecSys(\n",
       "  (d1): Dropout(p=0.1, inplace=False)\n",
       "  (fc1): Linear(in_features=1704, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc3): Linear(in_features=1024, out_features=1682, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mask = torch.load(\"../models/rating_bce_mask\")\n",
    "model_mask.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_output(\n",
    "    model: nn.Module,\n",
    "    input_data: np.ndarray,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        input_tensor = torch.Tensor([input_data]).to(DEVICE)\n",
    "        model_out = model(input_tensor)\n",
    "\n",
    "    return model_out[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_genres(path: str) -> list[str]:\n",
    "    return pd.read_csv(\n",
    "        os.path.join(path, \"u.genre\"),\n",
    "        sep=\"|\",\n",
    "        header=None,\n",
    "        names=[\"name\", \"genre_idx\"],\n",
    "        encoding=\"ISO-8859-1\",\n",
    "    )[\"name\"].tolist()\n",
    "\n",
    "\n",
    "def load_items(path: str, genres: list[str]) -> pd.DataFrame:\n",
    "    return pd.read_csv(\n",
    "        os.path.join(path, \"u.item\"),\n",
    "        sep=\"|\",\n",
    "        header=None,\n",
    "        names=[\n",
    "            \"movie_id\",\n",
    "            \"movie_title\",\n",
    "            \"release_date\",\n",
    "            \"video_release_date\",\n",
    "            \"IMDb_URL\",\n",
    "            *genres,\n",
    "        ],\n",
    "        encoding=\"ISO-8859-1\",\n",
    "    )\n",
    "\n",
    "\n",
    "genres = load_genres(\"../data/raw/ml-100k/\")\n",
    "movies_df = load_items(\"../data/raw/ml-100k/\", genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3, 0, 0, 0, 0, 5, 0])\n",
    "b = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, 40, 50, 60, 70,  0, 90])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[np.nonzero(a > 0)[0]] = 0\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unseen_on_input_data(\n",
    "    input_rating: np.ndarray, movie_ratings: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    unseen_ratings = movie_ratings.copy()\n",
    "    seen_indices = np.nonzero(input_rating > 0)[0]\n",
    "    unseen_ratings[seen_indices] = 0\n",
    "    return unseen_ratings\n",
    "\n",
    "\n",
    "def calculate_genre_ratios(\n",
    "    movie_indices: np.ndarray, items_df: pd.DataFrame\n",
    ") -> np.ndarray:\n",
    "    genres_sum = (\n",
    "        items_df[items_df[\"movie_id\"].isin(movie_indices + 1)]\n",
    "        .iloc[:, 5:]\n",
    "        .sum(axis=0)\n",
    "        .to_numpy()\n",
    "    )\n",
    "    return genres_sum / genres_sum.sum()\n",
    "\n",
    "\n",
    "def get_recommendations(\n",
    "    model: nn.Module,\n",
    "    encoded_age: float,\n",
    "    encoded_gender: int,\n",
    "    encoded_occupation: int,\n",
    "    movie_indices: list[int],\n",
    "    movies_df: pd.DataFrame,\n",
    "    predicted_threshold: float,\n",
    "    num_recs: int = 5,\n",
    ") -> np.ndarray:\n",
    "    movie_indices_shifted = np.array(movie_indices) - 1  # starting from 0\n",
    "\n",
    "    movies_ratings = np.zeros(NUM_MOVIES)\n",
    "    movies_ratings[movie_indices_shifted] = 1.0  # rating = 5\n",
    "    input_vector = np.array(\n",
    "        [\n",
    "            encoded_age,\n",
    "            encoded_gender,\n",
    "            encoded_occupation,\n",
    "            *calculate_genre_ratios(np.array(movie_indices_shifted), movies_df),\n",
    "            *movies_ratings,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    predictions = get_single_output(model, input_vector)\n",
    "    predictions[predictions < predicted_threshold] = 0.0\n",
    "    unseen_predictions = get_unseen_on_input_data(movies_ratings, predictions)\n",
    "\n",
    "    movie_ids = np.argsort(-unseen_predictions) + 1\n",
    "\n",
    "    unknown_idx = 267  # actual idx (from 1)\n",
    "    movie_ids = np.delete(movie_ids, np.where(movie_ids == unknown_idx))\n",
    "\n",
    "    return movie_ids[:num_recs]\n",
    "\n",
    "\n",
    "def get_movie_titles(\n",
    "    recommended_movies: np.ndarray, movies_df: pd.DataFrame\n",
    ") -> list[str]:\n",
    "    return [\n",
    "        movies_df[movies_df[\"movie_id\"] == movie_id][\"movie_title\"].to_list()[0]\n",
    "        for movie_id in recommended_movies\n",
    "    ]\n",
    "\n",
    "\n",
    "def show_recommendations(\n",
    "    models_set: list[tuple[str, nn.Module]],\n",
    "    movies_set: list[tuple[str, list[int]]],\n",
    "    predicted_threshold: float = 0.0,\n",
    "):\n",
    "    for movies_name, movies in movies_set:\n",
    "        print(movies_name)\n",
    "        for model_name, model in models_set:\n",
    "            recommended_movies = get_recommendations(\n",
    "                model, 0.21, 1, 19, movies, movies_df, predicted_threshold\n",
    "            )\n",
    "            print(f\"{model_name:10}: {get_movie_titles(recommended_movies, movies_df)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCI-FI\n",
      "No mask   : ['Star Trek VI: The Undiscovered Country (1991)', 'Evil Dead II (1987)', 'Godfather, The (1972)', 'Indiana Jones and the Last Crusade (1989)', 'Four Weddings and a Funeral (1994)']\n",
      "Mask      : ['Breaking the Waves (1996)', 'Lawrence of Arabia (1962)', 'Alien (1979)', 'Terminator, The (1984)', 'Raiders of the Lost Ark (1981)']\n",
      "\n",
      "CARTOONS\n",
      "No mask   : ['Contact (1997)', 'Raiders of the Lost Ark (1981)', 'Star Wars (1977)', 'Beavis and Butt-head Do America (1996)', 'Pulp Fiction (1994)']\n",
      "Mask      : ['Boogie Nights (1997)', 'Highlander (1986)', 'Star Wars (1977)', \"Ulee's Gold (1997)\", 'Die Hard (1988)']\n",
      "\n",
      "STAR TRACK\n",
      "No mask   : ['In the Name of the Father (1993)', 'Indiana Jones and the Last Crusade (1989)', 'Contact (1997)', 'Scream (1996)', 'Crumb (1994)']\n",
      "Mask      : ['Star Trek VI: The Undiscovered Country (1991)', 'Aliens (1986)', 'Lawrence of Arabia (1962)', 'Under Siege (1992)', 'Magnificent Seven, The (1954)']\n",
      "\n",
      "PULP FICTION\n",
      "No mask   : ['In the Name of the Father (1993)', 'Braveheart (1995)', '2001: A Space Odyssey (1968)', 'Hard Rain (1998)', 'Indiana Jones and the Last Crusade (1989)']\n",
      "Mask      : ['Boogie Nights (1997)', 'Liar Liar (1997)', 'Starship Troopers (1997)', 'Chasing Amy (1997)', 'Event Horizon (1997)']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models_set = [\n",
    "    (\"No mask\", model),\n",
    "    (\"Mask\", model_mask),\n",
    "]\n",
    "\n",
    "movies_set = [\n",
    "    (\n",
    "        \"SCI-FI\",\n",
    "        [50, 257, 204, 181],\n",
    "    ),  # Star Wars, MIB, Back to The Future, Return of the Jedi\n",
    "    (\"CARTOONS\", [1, 225, 465, 501]),  # Toy Story, 101 Dalmatians, Jungle Book, Dumbo\n",
    "    (\"STAR TRACK\", [222, 228, 380, 449]),  # Star Tracks\n",
    "    (\"PULP FICTION\", [56]),  # Pulp Fiction\n",
    "]\n",
    "\n",
    "show_recommendations(models_set, movies_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCI-FI\n",
      "No mask   : ['Last of the Mohicans, The (1992)', 'Four Weddings and a Funeral (1994)', 'Fugitive, The (1993)', 'Terminator 2: Judgment Day (1991)', 'True Lies (1994)']\n",
      "Mask      : ['Star Trek VI: The Undiscovered Country (1991)', '2001: A Space Odyssey (1968)', 'Highlander (1986)', 'Die Hard (1988)', \"Ulee's Gold (1997)\"]\n",
      "\n",
      "CARTOONS\n",
      "No mask   : ['Perfect World, A (1993)', 'U Turn (1997)', 'Star Trek III: The Search for Spock (1984)', 'Crumb (1994)', 'Star Trek VI: The Undiscovered Country (1991)']\n",
      "Mask      : ['Alien (1979)', 'Lawrence of Arabia (1962)', 'Highlander (1986)', '2001: A Space Odyssey (1968)', 'Boogie Nights (1997)']\n",
      "\n",
      "STAR TRACK\n",
      "No mask   : ['Alien (1979)', 'Terminator, The (1984)', 'Contact (1997)', 'Under Siege (1992)', 'U Turn (1997)']\n",
      "Mask      : ['Raiders of the Lost Ark (1981)', 'Star Trek III: The Search for Spock (1984)', 'Indiana Jones and the Last Crusade (1989)', 'Return of the Jedi (1983)', 'Event Horizon (1997)']\n",
      "\n",
      "PULP FICTION\n",
      "No mask   : ['2001: A Space Odyssey (1968)', 'Terminator 2: Judgment Day (1991)', 'True Lies (1994)', 'Godfather, The (1972)', 'Good Will Hunting (1997)']\n",
      "Mask      : ['Terminator, The (1984)', 'Starship Troopers (1997)', 'Event Horizon (1997)', 'Breaking the Waves (1996)', 'Contact (1997)']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_recommendations(models_set, movies_set, predicted_threshold=0.8)  # rating >= 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_data(\n",
    "    model: nn.Module, dataset: RecommendationDataset\n",
    ") -> list[tuple[np.ndarray, np.ndarray]]:\n",
    "    test_data = []\n",
    "\n",
    "    for input_data, _, target in tqdm(dataset):\n",
    "        predicted = get_single_output(model, input_data)\n",
    "\n",
    "        input_ratings = input_data[TOTAL_USER_FEATURES:]\n",
    "        unseen_predicted = get_unseen_on_input_data(input_ratings, predicted)\n",
    "        unseen_target = get_unseen_on_input_data(input_ratings, target)\n",
    "        test_data.append((unseen_target, unseen_predicted))\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_args(x: np.ndarray, n: int) -> np.ndarray:\n",
    "    return np.argsort(-x)[:n]\n",
    "\n",
    "\n",
    "def top_intersection(target: np.ndarray, predicted: np.ndarray, top_n: int = 20):\n",
    "    return list(\n",
    "        set(get_top_args(target, top_n)).intersection(get_top_args(predicted, top_n))\n",
    "    )\n",
    "\n",
    "\n",
    "def top_k_intersections(\n",
    "    data: list[tuple[np.ndarray, np.ndarray]], k: int, threshold: float = 0.0\n",
    ") -> list[int]:\n",
    "    intersections = []\n",
    "    for unseen_target, unseen_predicted in data:\n",
    "        nonzero_targets = unseen_target[unseen_target > threshold]\n",
    "        relevant_predicted = unseen_predicted[unseen_predicted > threshold]\n",
    "        intersections.append(\n",
    "            len(top_intersection(nonzero_targets, relevant_predicted, k))\n",
    "        )\n",
    "\n",
    "    return intersections\n",
    "\n",
    "\n",
    "def retrieval_precisions_on_k(\n",
    "    data: list[tuple[np.ndarray, np.ndarray]], k: int\n",
    ") -> list[int]:\n",
    "    retrieval_precisions = []\n",
    "    for unseen_target, unseen_predicted in data:\n",
    "        nonzero_targets = unseen_target > 0\n",
    "        relevant_predicted = unseen_predicted\n",
    "\n",
    "        retrieval_precisions.append(\n",
    "            retrieval_precision(\n",
    "                torch.Tensor(relevant_predicted), torch.Tensor(nonzero_targets), k\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return retrieval_precisions\n",
    "\n",
    "\n",
    "def average_precision_on_k(target: np.ndarray, predicted: np.ndarray, k: int) -> float:\n",
    "    relevant_predicted = predicted.copy()\n",
    "    if len(relevant_predicted) > k:\n",
    "        relevant_predicted = relevant_predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    hits = 0\n",
    "\n",
    "    for idx, x in enumerate(relevant_predicted):\n",
    "        if x in target and x not in relevant_predicted[:idx]:\n",
    "            hits += 1\n",
    "            score += hits / (idx + 1.0)\n",
    "\n",
    "    return score / min(len(target), k)\n",
    "\n",
    "\n",
    "def map_on_k(targets: list[np.ndarray], predictions: list[np.ndarray], k: int) -> float:\n",
    "    return np.mean(\n",
    "        [\n",
    "            average_precision_on_k(target, predicted, k)\n",
    "            for target, predicted in zip(targets, predictions)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_total_data_lists(\n",
    "    data: list[tuple[np.ndarray, np.ndarray]]\n",
    ") -> tuple[list[np.ndarray], list[np.ndarray]]:\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    for unseen_target, unseen_predicted in data:\n",
    "        nonzero_targets = unseen_target > 0\n",
    "        all_targets.append(\n",
    "            np.argsort(nonzero_targets)[len(nonzero_targets) - sum(nonzero_targets) :]\n",
    "        )\n",
    "        all_predictions.append(np.argsort(-unseen_predicted))\n",
    "\n",
    "    return all_targets, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(data: list[tuple[np.ndarray, np.ndarray]], ks: list[int]):\n",
    "    all_targets, all_predictions = generate_total_data_lists(data)\n",
    "    for k in ks:\n",
    "        print(f\"K={k}\")\n",
    "        intersections = top_k_intersections(data, k)\n",
    "        retrieval_precisions = retrieval_precisions_on_k(data, k)\n",
    "        map_score = map_on_k(all_targets, all_predictions, k)\n",
    "\n",
    "        print(f\"Mean top intersections: {np.mean(intersections)}\")\n",
    "        print(f\"Mean retrieval precision: {np.mean(retrieval_precisions)}\")\n",
    "        print(f\"MAP: {map_score}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2829/2829 [00:03<00:00, 735.68it/s]\n",
      "100%|██████████| 2829/2829 [00:04<00:00, 668.52it/s]\n"
     ]
    }
   ],
   "source": [
    "ks = [5, 10, 20, 50]\n",
    "\n",
    "test_data = generate_test_data(model, val_dataset)\n",
    "test_data_mask = generate_test_data(model_mask, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=5\n",
      "Mean top intersections: 0.11947684694238246\n",
      "Mean retrieval precision: 0.839802086353302\n",
      "MAP: 0.8038234947566866\n",
      "\n",
      "K=10\n",
      "Mean top intersections: 0.3407564510427713\n",
      "Mean retrieval precision: 0.7945916652679443\n",
      "MAP: 0.7420682191811566\n",
      "\n",
      "K=20\n",
      "Mean top intersections: 1.0420643336868152\n",
      "Mean retrieval precision: 0.7350654006004333\n",
      "MAP: 0.6761280522947515\n",
      "\n",
      "K=50\n",
      "Mean top intersections: 4.363379285966773\n",
      "Mean retrieval precision: 0.6175397634506226\n",
      "MAP: 0.6387461661720604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_metrics(test_data, ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=5\n",
      "Mean top intersections: 0.11134676564156946\n",
      "Mean retrieval precision: 0.7874161005020142\n",
      "MAP: 0.7343831742665253\n",
      "\n",
      "K=10\n",
      "Mean top intersections: 0.3336868151290209\n",
      "Mean retrieval precision: 0.7383174300193787\n",
      "MAP: 0.669376525442273\n",
      "\n",
      "K=20\n",
      "Mean top intersections: 1.1081654294803818\n",
      "Mean retrieval precision: 0.6741604804992676\n",
      "MAP: 0.5986471207347243\n",
      "\n",
      "K=50\n",
      "Mean top intersections: 4.361258395192648\n",
      "Mean retrieval precision: 0.5633792877197266\n",
      "MAP: 0.552120947808151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_metrics(test_data_mask, ks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
